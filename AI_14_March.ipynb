{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4we5R-wLDrGd",
        "outputId": "6ecad3ae-e3bf-4472-8aac-9e6b6a3f52b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "160/160 [==============================] - 2s 5ms/step - loss: 7096.0933 - val_loss: 51.8271\n",
            "Epoch 2/50\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 30.0189 - val_loss: 23.6429\n",
            "Epoch 3/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 14.3159 - val_loss: 10.3757\n",
            "Epoch 4/50\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 7.9858 - val_loss: 6.1627\n",
            "Epoch 5/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 6.0746 - val_loss: 4.7679\n",
            "Epoch 6/50\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 5.2816 - val_loss: 4.0406\n",
            "Epoch 7/50\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 4.7699 - val_loss: 3.6314\n",
            "Epoch 8/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 4.3521 - val_loss: 3.2435\n",
            "Epoch 9/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 3.9623 - val_loss: 2.8772\n",
            "Epoch 10/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 3.6030 - val_loss: 2.5994\n",
            "Epoch 11/50\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 3.2731 - val_loss: 2.3390\n",
            "Epoch 12/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 3.0407 - val_loss: 2.1603\n",
            "Epoch 13/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 2.8103 - val_loss: 2.0411\n",
            "Epoch 14/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 2.6555 - val_loss: 2.0027\n",
            "Epoch 15/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 2.5653 - val_loss: 1.9341\n",
            "Epoch 16/50\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 2.3988 - val_loss: 2.0137\n",
            "Epoch 17/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 2.1271 - val_loss: 1.8046\n",
            "Epoch 18/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 1.9223 - val_loss: 1.7395\n",
            "Epoch 19/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 1.8180 - val_loss: 1.6045\n",
            "Epoch 20/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 1.6850 - val_loss: 1.6240\n",
            "Epoch 21/50\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.6520 - val_loss: 1.5735\n",
            "Epoch 22/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 1.5977 - val_loss: 1.5565\n",
            "Epoch 23/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 1.5804 - val_loss: 1.5478\n",
            "Epoch 24/50\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.5330 - val_loss: 1.6514\n",
            "Epoch 25/50\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.5200 - val_loss: 1.5481\n",
            "Epoch 26/50\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.5029 - val_loss: 1.5225\n",
            "Epoch 27/50\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 1.4857 - val_loss: 1.6469\n",
            "Epoch 28/50\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 1.4684 - val_loss: 1.5047\n",
            "Epoch 29/50\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 1.4993 - val_loss: 1.6066\n",
            "Epoch 30/50\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 1.4809 - val_loss: 1.5506\n",
            "Epoch 31/50\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 1.4832 - val_loss: 1.4749\n",
            "Epoch 32/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 1.4933 - val_loss: 1.9337\n",
            "Epoch 33/50\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.4224 - val_loss: 1.6236\n",
            "Epoch 34/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 1.4989 - val_loss: 1.4631\n",
            "Epoch 35/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 1.4162 - val_loss: 1.4692\n",
            "Epoch 36/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 1.4419 - val_loss: 1.4433\n",
            "Epoch 37/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 1.4408 - val_loss: 1.4540\n",
            "Epoch 38/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 1.4306 - val_loss: 1.3852\n",
            "Epoch 39/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 1.4366 - val_loss: 1.4693\n",
            "Epoch 40/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 1.4251 - val_loss: 1.5799\n",
            "Epoch 41/50\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.3779 - val_loss: 1.5167\n",
            "Epoch 42/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 1.4354 - val_loss: 1.8957\n",
            "Epoch 43/50\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.4476 - val_loss: 1.7551\n",
            "Epoch 44/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 1.5560 - val_loss: 1.6516\n",
            "Epoch 45/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 1.3926 - val_loss: 1.4414\n",
            "Epoch 46/50\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.3935 - val_loss: 1.3542\n",
            "Epoch 47/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 1.4287 - val_loss: 1.5263\n",
            "Epoch 48/50\n",
            "160/160 [==============================] - 0s 3ms/step - loss: 1.3764 - val_loss: 1.5915\n",
            "Epoch 49/50\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.3810 - val_loss: 2.9445\n",
            "Epoch 50/50\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 1.4876 - val_loss: 1.3686\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 1.2353\n",
            "Test loss: 1.24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f567ac05310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 88ms/step\n",
            "[[2.634779]]\n",
            "Prediction: 2.63\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "# Load the ChEMBL dataset\n",
        "df = pd.read_csv('/content/chembl_dataset_7956_smiles.csv')\n",
        " \n",
        "# Define the input features and output feature\n",
        "x_features = 4\n",
        "y_feature = 'logP'\n",
        " \n",
        "# Extract the input features from the SMILES strings\n",
        "def extract_features(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    features = [\n",
        "        Descriptors.MolWt(mol),\n",
        "        Descriptors.TPSA(mol),\n",
        "        Descriptors.NumRotatableBonds(mol),\n",
        "        mol.GetRingInfo().NumRings()\n",
        "    ]\n",
        "    return np.array(features)\n",
        "\n",
        "def extract_output_feature(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    logp = Chem.rdMolDescriptors.CalcCrippenDescriptors(mol)[0]\n",
        "    return logp\n",
        " \n",
        "df['features'] = df['smiles'].apply(extract_features)\n",
        "\n",
        "# Remove rows with missing features\n",
        "df.dropna(subset=['features'], inplace=True)\n",
        "\n",
        "# adding output:\n",
        "df[y_feature] = df['smiles'].apply(extract_output_feature)\n",
        "\n",
        "df.dropna(subset=[y_feature], inplace=True)\n",
        "# print(df)\n",
        " \n",
        "# num_atoms = mol.GetNumAtoms()  # number of atoms\n",
        "\n",
        "# Extract the output feature as the target variable\n",
        "y = df[y_feature]\n",
        " \n",
        "# Convert the input features to a NumPy array\n",
        "x = np.stack(df['features'])\n",
        " \n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "\n",
        "# Define the architecture of the neural network\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(10, input_shape=(x_features,), activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        " \n",
        "# Compile the model\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        " \n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=50, validation_split=0.2)\n",
        " \n",
        "# Evaluate the model on the testing set\n",
        "loss = model.evaluate(x_test, y_test)\n",
        "print(f'Test loss: {loss:.2f}')\n",
        " \n",
        "# Make a prediction using the trained model\n",
        "x_new = np.array([[300.0, 50.0, 4, 1]])  # example input features\n",
        "y_new = model.predict(x_new)\n",
        "print(y_new)\n",
        "print(f'Prediction: {y_new[0][0]:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEac12z5JHRg",
        "outputId": "55cfa9ab-02da-4a86-cc1c-f780a5211448"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[439.559  67.87    5.      4.   ]\n",
            " [471.535 127.25    6.      4.   ]\n",
            " [630.69  130.12    5.      8.   ]\n",
            " ...\n",
            " [410.911 103.56    5.      3.   ]\n",
            " [377.404  83.32    6.      4.   ]\n",
            " [251.307  52.33    0.      3.   ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from rdkit import Chem\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# # Load Chembl dataset\n",
        "# df = pd.read_csv('/content/chembl_dataset_7956_smiles.csv')\n",
        "\n",
        "# # Extract features from SMILES representation\n",
        "# mol_list = [Chem.MolFromSmiles(smiles) for smiles in df['smiles']]\n",
        "# # df['num_atoms'] = [mol.GetNumAtoms() for mol in mol_list]\n",
        "# df['mol_weight'] = [Descriptors.MolWt(mol) for mol in mol_list]\n",
        "# df['num_rings'] = [mol.GetRingInfo().NumRings() for mol in mol_list]\n",
        "# # df['num_hbd'] = [Chem.rdMolDescriptors.CalcNumLipinskiHBD(mol) for mol in mol_list]\n",
        "# # df['num_hba'] = [Chem.rdMolDescriptors.CalcNumLipinskiHBA(mol) for mol in mol_list]\n",
        "# df['rotable_bonds'] = [Descriptors.NumRotatableBonds(mol) for mol in mol_list]\n",
        "# df['TPSA'] = [Descriptors.TPSA(mol) for mol in mol_list]\n",
        "# df['logP'] = [Chem.rdMolDescriptors.CalcCrippenDescriptors(mol)[0] for mol in mol_list]\n",
        "\n",
        "# # Define input and output features\n",
        "# input_features = ['TPSA', 'mol_weight', 'num_rings', 'rotable_bonds']\n",
        "# output_features = ['logP']\n",
        "\n",
        "# # Split dataset into train and test sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df[input_features], df[output_features], test_size=0.2, random_state=42)\n",
        "\n",
        "# # Standardize input features\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_test = scaler.transform(X_test)\n",
        "\n",
        "# Load the ChEMBL dataset\n",
        "df = pd.read_csv('/content/chembl_dataset_7956_smiles.csv')\n",
        " \n",
        "# Define the input features and output feature\n",
        "x_features = 4\n",
        "y_feature = 'logP'\n",
        " \n",
        "# Extract the input features from the SMILES strings\n",
        "def extract_features(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    features = [\n",
        "        Descriptors.MolWt(mol),\n",
        "        Descriptors.TPSA(mol),\n",
        "        Descriptors.NumRotatableBonds(mol),\n",
        "        mol.GetRingInfo().NumRings()\n",
        "    ]\n",
        "    return np.array(features)\n",
        "\n",
        "def extract_output_feature(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    logp = Chem.rdMolDescriptors.CalcCrippenDescriptors(mol)[0]\n",
        "    return logp\n",
        " \n",
        "df['features'] = df['smiles'].apply(extract_features)\n",
        "\n",
        "# Remove rows with missing features\n",
        "df.dropna(subset=['features'], inplace=True)\n",
        "\n",
        "# adding output:\n",
        "df[y_feature] = df['smiles'].apply(extract_output_feature)\n",
        "\n",
        "df.dropna(subset=[y_feature], inplace=True)\n",
        "# print(df)\n",
        " \n",
        "# num_atoms = mol.GetNumAtoms()  # number of atoms\n",
        "\n",
        "# Extract the output feature as the target variable\n",
        "y = df[y_feature]\n",
        " \n",
        "# Convert the input features to a NumPy array\n",
        "x = np.stack(df['features'])\n",
        " \n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Create neural network model\n",
        "def create_nn_model(input_shape):\n",
        "    inp = Input(shape=(input_shape,))\n",
        "    x = Dense(256)(inp)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.05)(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = Dense(1024)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.05)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(1024)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.05)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(512)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.05)(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = Dense(512)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.05)(x)\n",
        "    x = Dense(256)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.05)(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    out = Dense(1, activation=\"linear\")(x)\n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    return model\n",
        "\n",
        "# Train neural network model\n",
        "model = create_nn_model(X_train.shape[1])\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "history = model.fit(X_train, y_train, batch_size=128, epochs=50, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate neural network model\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Test loss: {loss:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdw_VpTUWf_-",
        "outputId": "ba6d9f1e-63bf-46f2-fd60-cb44dbfb52d5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "50/50 [==============================] - 10s 98ms/step - loss: 5.1826 - mae: 1.6544 - val_loss: 6.8490 - val_mae: 2.1820\n",
            "Epoch 2/50\n",
            "50/50 [==============================] - 5s 100ms/step - loss: 2.7837 - mae: 1.1275 - val_loss: 3.2631 - val_mae: 1.3441\n",
            "Epoch 3/50\n",
            "50/50 [==============================] - 5s 106ms/step - loss: 2.3260 - mae: 1.0816 - val_loss: 3.1285 - val_mae: 1.3416\n",
            "Epoch 4/50\n",
            "50/50 [==============================] - 5s 92ms/step - loss: 2.3108 - mae: 1.0604 - val_loss: 2.4700 - val_mae: 1.1432\n",
            "Epoch 5/50\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 2.3557 - mae: 1.0622 - val_loss: 2.2009 - val_mae: 1.0709\n",
            "Epoch 6/50\n",
            "50/50 [==============================] - 5s 91ms/step - loss: 2.0451 - mae: 1.0345 - val_loss: 2.1835 - val_mae: 1.0712\n",
            "Epoch 7/50\n",
            "50/50 [==============================] - 5s 91ms/step - loss: 1.8934 - mae: 1.0072 - val_loss: 1.5883 - val_mae: 0.9167\n",
            "Epoch 8/50\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 1.8186 - mae: 0.9844 - val_loss: 1.7759 - val_mae: 0.9725\n",
            "Epoch 9/50\n",
            "50/50 [==============================] - 5s 91ms/step - loss: 1.8395 - mae: 0.9876 - val_loss: 1.6457 - val_mae: 0.9526\n",
            "Epoch 10/50\n",
            "50/50 [==============================] - 5s 99ms/step - loss: 1.7377 - mae: 0.9535 - val_loss: 1.5436 - val_mae: 0.9196\n",
            "Epoch 11/50\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 1.7119 - mae: 0.9582 - val_loss: 1.4534 - val_mae: 0.8881\n",
            "Epoch 12/50\n",
            "50/50 [==============================] - 4s 89ms/step - loss: 1.6435 - mae: 0.9380 - val_loss: 1.4526 - val_mae: 0.8658\n",
            "Epoch 13/50\n",
            "50/50 [==============================] - 6s 121ms/step - loss: 1.6620 - mae: 0.9532 - val_loss: 1.4016 - val_mae: 0.8715\n",
            "Epoch 14/50\n",
            "50/50 [==============================] - 4s 90ms/step - loss: 1.6019 - mae: 0.9332 - val_loss: 1.3590 - val_mae: 0.8747\n",
            "Epoch 15/50\n",
            "50/50 [==============================] - 4s 89ms/step - loss: 1.6304 - mae: 0.9326 - val_loss: 1.3398 - val_mae: 0.8463\n",
            "Epoch 16/50\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 1.5737 - mae: 0.9208 - val_loss: 1.4794 - val_mae: 0.8616\n",
            "Epoch 17/50\n",
            "50/50 [==============================] - 4s 89ms/step - loss: 1.5674 - mae: 0.9134 - val_loss: 1.3781 - val_mae: 0.8521\n",
            "Epoch 18/50\n",
            "50/50 [==============================] - 5s 99ms/step - loss: 1.5666 - mae: 0.9153 - val_loss: 1.3224 - val_mae: 0.8403\n",
            "Epoch 19/50\n",
            "50/50 [==============================] - 5s 108ms/step - loss: 1.5254 - mae: 0.9114 - val_loss: 1.8019 - val_mae: 0.8796\n",
            "Epoch 20/50\n",
            "50/50 [==============================] - 5s 91ms/step - loss: 1.4784 - mae: 0.9002 - val_loss: 1.4433 - val_mae: 0.8769\n",
            "Epoch 21/50\n",
            "50/50 [==============================] - 6s 119ms/step - loss: 1.5485 - mae: 0.9071 - val_loss: 1.3471 - val_mae: 0.8513\n",
            "Epoch 22/50\n",
            "50/50 [==============================] - 5s 106ms/step - loss: 1.5150 - mae: 0.8966 - val_loss: 1.3623 - val_mae: 0.8469\n",
            "Epoch 23/50\n",
            "50/50 [==============================] - 6s 112ms/step - loss: 1.4907 - mae: 0.9001 - val_loss: 1.3838 - val_mae: 0.8523\n",
            "Epoch 24/50\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 1.4743 - mae: 0.8901 - val_loss: 1.3026 - val_mae: 0.8413\n",
            "Epoch 25/50\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 1.5333 - mae: 0.8986 - val_loss: 1.5991 - val_mae: 0.8706\n",
            "Epoch 26/50\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 1.4373 - mae: 0.8833 - val_loss: 1.2789 - val_mae: 0.8361\n",
            "Epoch 27/50\n",
            "50/50 [==============================] - 5s 90ms/step - loss: 1.4425 - mae: 0.8898 - val_loss: 1.4015 - val_mae: 0.8652\n",
            "Epoch 28/50\n",
            "50/50 [==============================] - 5s 92ms/step - loss: 1.4814 - mae: 0.8941 - val_loss: 1.2775 - val_mae: 0.8314\n",
            "Epoch 29/50\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 1.4295 - mae: 0.8831 - val_loss: 1.3260 - val_mae: 0.8508\n",
            "Epoch 30/50\n",
            "50/50 [==============================] - 5s 91ms/step - loss: 1.4241 - mae: 0.8845 - val_loss: 1.3720 - val_mae: 0.8513\n",
            "Epoch 31/50\n",
            "50/50 [==============================] - 5s 106ms/step - loss: 1.4657 - mae: 0.8850 - val_loss: 1.3791 - val_mae: 0.8592\n",
            "Epoch 32/50\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 1.4158 - mae: 0.8802 - val_loss: 1.2700 - val_mae: 0.8341\n",
            "Epoch 33/50\n",
            "50/50 [==============================] - 5s 91ms/step - loss: 1.4158 - mae: 0.8757 - val_loss: 1.3560 - val_mae: 0.8477\n",
            "Epoch 34/50\n",
            "50/50 [==============================] - 6s 117ms/step - loss: 1.4823 - mae: 0.8898 - val_loss: 1.2728 - val_mae: 0.8350\n",
            "Epoch 35/50\n",
            "50/50 [==============================] - 5s 91ms/step - loss: 1.4469 - mae: 0.8855 - val_loss: 1.3914 - val_mae: 0.8559\n",
            "Epoch 36/50\n",
            "50/50 [==============================] - 5s 93ms/step - loss: 1.4103 - mae: 0.8694 - val_loss: 1.4099 - val_mae: 0.8445\n",
            "Epoch 37/50\n",
            "50/50 [==============================] - 6s 118ms/step - loss: 1.4073 - mae: 0.8640 - val_loss: 1.3693 - val_mae: 0.8422\n",
            "Epoch 38/50\n",
            "50/50 [==============================] - 5s 92ms/step - loss: 1.3998 - mae: 0.8686 - val_loss: 1.2884 - val_mae: 0.8414\n",
            "Epoch 39/50\n",
            "50/50 [==============================] - 5s 105ms/step - loss: 1.3745 - mae: 0.8734 - val_loss: 1.3763 - val_mae: 0.8483\n",
            "Epoch 40/50\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 1.4066 - mae: 0.8702 - val_loss: 1.3270 - val_mae: 0.8512\n",
            "Epoch 41/50\n",
            "50/50 [==============================] - 5s 90ms/step - loss: 1.3784 - mae: 0.8611 - val_loss: 1.4038 - val_mae: 0.8506\n",
            "Epoch 42/50\n",
            "50/50 [==============================] - 6s 118ms/step - loss: 1.4721 - mae: 0.8855 - val_loss: 1.3875 - val_mae: 0.8515\n",
            "Epoch 43/50\n",
            "50/50 [==============================] - 5s 91ms/step - loss: 1.3728 - mae: 0.8684 - val_loss: 1.4036 - val_mae: 0.8432\n",
            "Epoch 44/50\n",
            "50/50 [==============================] - 4s 89ms/step - loss: 1.4190 - mae: 0.8689 - val_loss: 1.2737 - val_mae: 0.8429\n",
            "Epoch 45/50\n",
            "50/50 [==============================] - 6s 116ms/step - loss: 1.3560 - mae: 0.8568 - val_loss: 1.4854 - val_mae: 0.8561\n",
            "Epoch 46/50\n",
            "50/50 [==============================] - 5s 92ms/step - loss: 1.4047 - mae: 0.8693 - val_loss: 1.4961 - val_mae: 0.8666\n",
            "Epoch 47/50\n",
            "50/50 [==============================] - 5s 108ms/step - loss: 1.3979 - mae: 0.8671 - val_loss: 1.2698 - val_mae: 0.8367\n",
            "Epoch 48/50\n",
            "50/50 [==============================] - 5s 99ms/step - loss: 1.3685 - mae: 0.8682 - val_loss: 1.4921 - val_mae: 0.8507\n",
            "Epoch 49/50\n",
            "50/50 [==============================] - 5s 92ms/step - loss: 1.3772 - mae: 0.8544 - val_loss: 1.4049 - val_mae: 0.8635\n",
            "Epoch 50/50\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 1.3513 - mae: 0.8581 - val_loss: 1.2924 - val_mae: 0.8368\n",
            "50/50 [==============================] - 1s 12ms/step - loss: 1.2924 - mae: 0.8368\n",
            "Test loss: 1.29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_new = np.array(scaler.transform([[300.0, 50.0, 4, 1]]))  # example input features\n",
        "y_new = model.predict(x_new)\n",
        "print(y_new)\n",
        "print(f'Prediction: {y_new[0][0]:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz2KPCR7bCZa",
        "outputId": "d24b63b4-e468-4693-e901-99f29acec561"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 53ms/step\n",
            "[[3.6608336]]\n",
            "Prediction: 3.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m13MsbUwbcZU",
        "outputId": "4f5dc0d5-a50a-4e70-f01b-5beb150229d4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.39915238  0.02333863 -0.30487919  0.34093125]\n",
            " [ 0.15608182  0.74650347  0.05090171 -0.4041534 ]\n",
            " [-0.05591928  2.66987773 -1.016441   -2.63940735]\n",
            " ...\n",
            " [-0.3415546  -0.03473164  0.05090171 -0.4041534 ]\n",
            " [ 0.44520479  0.28166994 -0.12698874  1.83110056]\n",
            " [ 0.08031626 -0.36076637 -0.30487919  1.0860159 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gTWEWr3Tbch7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}